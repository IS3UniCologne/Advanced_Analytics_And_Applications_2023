{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop: Deep Learning 3\n",
    "\n",
    "Outline\n",
    "1. Regularization\n",
    "2. Gradient Descent\n",
    "3. Hand-Written Digits with Convolutional Neural Networks \n",
    "4. Advanced Image Classification with Convolutional Neural Networks \n",
    "\n",
    "\n",
    "Source: Deep Learning With Python, Part 1 - Chapter 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prevent a model from learning misleading or irrelevant patterns found in the\n",
    "training data, the best solution is to get more training data. However, this is in many times out of our control.\n",
    "\n",
    "Another approach is called - by now you should know that - regularization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Reducing the networkâ€™s size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to prevent overfitting is to reduce the size of the model: the number\n",
    "of learnable parameters in the model (which is determined by the number of layers\n",
    "and the number of units per layer).\n",
    "\n",
    "Or put it this way: A network with more parameters can better memorize stuff..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfortunately, there is no closed form solution which gives us the best network size...\n",
    "# So, we need to try out different models (or use grid search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original  Model \n",
    "from keras import models\n",
    "from keras import layers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpler Model \n",
    "from keras import models\n",
    "from keras import layers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(8, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(8, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigger Model \n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(512, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### You need to load data, compile the network and then train it (with validation/hold out set)\n",
    "#### Then you plot the validation loss for all these combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"res/img1.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"res/img2.png\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shows us that the bigger model starts to overfit immediately.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of manually searching for the best model architecture (i.e., hyperparameters) you can use a method called grid-search. However, we will not cover this in this lecture - but you can find a tutorial here:\n",
    "\n",
    "https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n",
    "\n",
    "\n",
    "Basically, the author conceleates keras with scikit's grid search module. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Adding weight regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. L1 regularization\n",
    "2. L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Adding L2 Regularization to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "model = models.Sequential()\n",
    "\n",
    "# kernel_regularizer = regularizers.l2(0.001), add those weights to the loss with an alpha of 0.001\n",
    "# you could use also: regularizers.l1(0.001) for L1 regularization\n",
    "# Documentation: https://keras.io/api/layers/regularizers/\n",
    "model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),activation='relu', input_shape=(10000,)))\n",
    "\n",
    "model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001), activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"res/img3.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Adding Dropout "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: Randomly drop out a number of (activation) nodes during training. \n",
    "    \n",
    "**Assume**: [0.2, 0.5, 1.3, 0.8, 1.1] is the output of a layer (after activation function).\n",
    "\n",
    "Dropout sets randomly some of these weights to 0. For example: [0, 0.5, 1.3, 0, 1.1]. \n",
    "\n",
    "The *dropout rate* is the fraction of features that are zeroed out (usually between 0.2 and 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Code \n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "\n",
    "# Pass dropout rate!!!\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile..\n",
    "# Fit..\n",
    "# Evaluate...\n",
    "# Doc: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"res/img4.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To recap, these are the most common ways to prevent overfitting in neural networks:\n",
    "1. Get more training data.\n",
    "2. Reduce the capacity of the network.\n",
    "3. Add weight regularization.\n",
    "4. Add dropout.\n",
    "5. Data Augmentation (for image classification tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Gradient Descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets \n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "housing_data = datasets.fetch_california_housing()\n",
    "\n",
    "features = pd.DataFrame(housing_data.data, columns=housing_data.feature_names)\n",
    "target = pd.DataFrame(housing_data.target, columns=['Target'])\n",
    "\n",
    "df = features.join(target)\n",
    "\n",
    "X = df.MedInc\n",
    "Y = df.Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = mx+b\n",
    "# MSE 1/N * sum( (y_i - (m * x_i +b))^2) <= Loss Function\n",
    "\n",
    "def gradient_descent(X, y, lr=0.05, iterations=10):\n",
    "    \n",
    "    '''\n",
    "    Gradient Descent for a single feature\n",
    "    '''\n",
    "    \n",
    "    m, b = 0.2, 0.2 # initial random parameters\n",
    "    log, mse = [], [] # lists to store learning process\n",
    "    N = len(X) # number of samples\n",
    "    \n",
    "    # MSE = 1/N SUM (y_i - (m*x_i +b))^2 \n",
    "    # MSE' w.r.t. m => 1/N * SUM(-2*x_i*(m*x_i+b))\n",
    "    # MSE' w.r.t. b => 1/N * SUM(-2*(m*x_i+b))\n",
    "\n",
    "    for _ in range(iterations):\n",
    "                \n",
    "        f = y - (m*X + b)\n",
    "    \n",
    "        # Updating m and b \n",
    "        m -= lr * (-2 * X.dot(f).sum() / N) \n",
    "        b -= lr * (-2 * f.sum() / N)\n",
    "        \n",
    "        log.append((m, b))\n",
    "        mse.append(mean_squared_error(y, (m*X + b)))        \n",
    "    \n",
    "    return m, b, log, mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, b, log, mse = gradient_descent(X, Y, lr=0.01, iterations=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.41893244701097204, 0.44612945637258383)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(m, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analytical Solution (compaed to )\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression().fit(features[\"MedInc\"].to_numpy().reshape(-1, 1), Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.41793849]), 0.4508557670326776)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(reg.coef_, reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(X, y, lr=0.05, iterations=10, batch_size=10):\n",
    "        \n",
    "    '''\n",
    "    Stochastic Gradient Descent for a single feature\n",
    "    '''\n",
    "    \n",
    "    m, b = 0.5, 0.5 # initial parameters\n",
    "    log, mse = [], [] # lists to store learning process\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        \n",
    "        indexes = np.random.randint(0, len(X), batch_size) # random sample \"batch_size\" elements from training set\n",
    "        \n",
    "        Xs = np.take(X, indexes)\n",
    "        ys = np.take(y, indexes)\n",
    "        N = len(Xs)\n",
    "        \n",
    "        f = ys - (m*Xs + b)\n",
    "        \n",
    "        # Updating parameters m and b\n",
    "        m -= lr * (-2 * Xs.dot(f).sum() / N)\n",
    "        b -= lr * (-2 * f.sum() / N)\n",
    "        \n",
    "        log.append((m, b))\n",
    "        mse.append(mean_squared_error(y, m*X+b))        \n",
    "    \n",
    "    return m, b, log, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, b, log, mse = stochastic_gradient_descent(X, Y, lr=0.01, iterations=100, batch_size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.41078958000903365, 0.4757463276058315)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(m,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using CNNs to Classify Hand-written Digits on MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"res/img5.png\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPool2D\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (60000, 28, 28)\n",
      "y_train shape (60000,)\n",
      "X_test shape (10000, 28, 28)\n",
      "y_test shape (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Shape of data\n",
    "print(\"X_train shape\", X_train.shape)\n",
    "print(\"y_train shape\", y_train.shape)\n",
    "print(\"X_test shape\", X_test.shape)\n",
    "print(\"y_test shape\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flattening the images from the 28x28 pixels to 1D 784 pixels\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the data to help with the training\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before one-hot encoding:  (60000,)\n",
      "Shape after one-hot encoding:  (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "# To Categorical (One-Hot Encoding)\n",
    "n_classes = 10\n",
    "print(\"Shape before one-hot encoding: \", y_train.shape)\n",
    "Y_train = np_utils.to_categorical(y_train, n_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, n_classes)\n",
    "print(\"Shape after one-hot encoding: \", Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build again a very boring neural network\n",
    "model = Sequential()\n",
    "# hidden layer\n",
    "model.add(Dense(100, input_shape=(784,), activation='relu'))\n",
    "# output layer\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_15 (Dense)            (None, 100)               78500     \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 79,510\n",
      "Trainable params: 79,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.3932 - accuracy: 0.8920 - val_loss: 0.2128 - val_accuracy: 0.9399\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 0s 1ms/step - loss: 0.1816 - accuracy: 0.9478 - val_loss: 0.1568 - val_accuracy: 0.9540\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 0s 1ms/step - loss: 0.1298 - accuracy: 0.9627 - val_loss: 0.1175 - val_accuracy: 0.9642\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.1021 - accuracy: 0.9704 - val_loss: 0.1069 - val_accuracy: 0.9682\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0837 - accuracy: 0.9754 - val_loss: 0.0949 - val_accuracy: 0.9712\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 0s 1ms/step - loss: 0.0693 - accuracy: 0.9796 - val_loss: 0.0822 - val_accuracy: 0.9747\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 0s 1ms/step - loss: 0.0587 - accuracy: 0.9833 - val_loss: 0.0820 - val_accuracy: 0.9731\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0509 - accuracy: 0.9853 - val_loss: 0.0756 - val_accuracy: 0.9767\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0440 - accuracy: 0.9876 - val_loss: 0.0777 - val_accuracy: 0.9769\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 0s 1ms/step - loss: 0.0381 - accuracy: 0.9887 - val_loss: 0.0777 - val_accuracy: 0.9764\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff55f759af0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at the model summary\n",
    "model.summary()\n",
    "# Compile\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "# Traing (####-> Caution, this is dedicated for validation data - I was just lazy...)\n",
    "model.fit(X_train, Y_train, batch_size=128, epochs=10, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 9s 19ms/step - loss: 0.1923 - accuracy: 0.9435 - val_loss: 0.0733 - val_accuracy: 0.9756\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 9s 18ms/step - loss: 0.0598 - accuracy: 0.9822 - val_loss: 0.0570 - val_accuracy: 0.9807\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 9s 19ms/step - loss: 0.0364 - accuracy: 0.9890 - val_loss: 0.0561 - val_accuracy: 0.9818\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 9s 20ms/step - loss: 0.0223 - accuracy: 0.9934 - val_loss: 0.0513 - val_accuracy: 0.9834\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 9s 19ms/step - loss: 0.0134 - accuracy: 0.9962 - val_loss: 0.0718 - val_accuracy: 0.9791\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 9s 18ms/step - loss: 0.0101 - accuracy: 0.9972 - val_loss: 0.0568 - val_accuracy: 0.9834\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 9s 18ms/step - loss: 0.0088 - accuracy: 0.9972 - val_loss: 0.0598 - val_accuracy: 0.9821\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 8s 18ms/step - loss: 0.0067 - accuracy: 0.9982 - val_loss: 0.0601 - val_accuracy: 0.9841\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 9s 19ms/step - loss: 0.0035 - accuracy: 0.9991 - val_loss: 0.0597 - val_accuracy: 0.9845\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 10s 22ms/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 0.0615 - val_accuracy: 0.9845\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff5885b5d60>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new imports needed\n",
    "from keras.layers import  Conv2D, MaxPool2D, Flatten\n",
    "\n",
    "# And now with a convolutional neural network\n",
    "# Doc: https://keras.io/api/layers/convolution_layers/\n",
    "\n",
    "# Load again data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# DONT Vectorize - keep grid structure\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# Sequential Model\n",
    "model = Sequential()\n",
    "# Convolutional layer\n",
    "\n",
    "# 2D convolutional data \n",
    "# filters: number of kernels\n",
    "# kernel size: (3, 3) pixel filter\n",
    "# stride: (move one to the right, one to the bottom when you reach the end of the row)\n",
    "# padding: \"valid\" => no padding => feature map is reduced\n",
    "model.add(Conv2D(filters=25, kernel_size=(3,3), strides=(1,1), padding='valid', activation='relu', input_shape=(28,28,1)))\n",
    "\n",
    "\n",
    "model.add(MaxPool2D(pool_size=(1,1)))\n",
    "# flatten output such that the \"densly\" connected network can be attached\n",
    "model.add(Flatten())\n",
    "\n",
    "# hidden layer\n",
    "model.add(Dense(100, activation='relu'))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# compiling the sequential model\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "# training the model for 10 epochs\n",
    "model.fit(X_train, Y_train, batch_size=128, epochs=10, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More on Classification with CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Image Classification with Deep Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"res/img6.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 20s 0us/step\n",
      "Shape before one-hot encoding:  (50000, 1)\n",
      "Shape after one-hot encoding:  (50000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# Load Data\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# # Keep Grid Structure with 32x32 pixels (times 3; due to color channels)\n",
    "X_train = X_train.reshape(X_train.shape[0], 32, 32, 3)\n",
    "X_test = X_test.reshape(X_test.shape[0], 32, 32, 3)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# One-Hot Encoding\n",
    "n_classes = 10\n",
    "print(\"Shape before one-hot encoding: \", y_train.shape)\n",
    "Y_train = np_utils.to_categorical(y_train, n_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, n_classes)\n",
    "print(\"Shape after one-hot encoding: \", Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "104/391 [======>.......................] - ETA: 1:31 - loss: 2.0036 - accuracy: 0.2529"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/l4/9h6cyd6s3yv14f4p2mpw0g0r0000gn/T/ipykernel_2516/4120395991.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 _r=1):\n\u001b[1;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2451\u001b[0m       (graph_function,\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2454\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1860\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1861\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    498\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Create Model Object\n",
    "model = Sequential()\n",
    "\n",
    "# Add Conv. Layer\n",
    "model.add(Conv2D(50, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu', input_shape=(32, 32, 3)))\n",
    "\n",
    "## What happens here?\n",
    "\n",
    "# Stack 2. Conv. Layer\n",
    "model.add(Conv2D(75, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\n",
    "\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Stack 3. Conv. Layer\n",
    "model.add(Conv2D(125, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Flatten Output of Conv. Part such that we can add a densly connected network\n",
    "model.add(Flatten()) \n",
    "\n",
    "# Add Hidden Layer and Dropout Reg.\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Output Layer\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "# Train\n",
    "model.fit(X_train, Y_train, batch_size=128, epochs=2, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
