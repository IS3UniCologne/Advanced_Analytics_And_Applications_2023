{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 6 \n",
    "\n",
    "### Outline: \n",
    " \n",
    "1. Multi-Class Classifcation: Classifying newswires (Chapter 3)\n",
    "2. Regression with Deep Learning (Chapter 3)\n",
    "\n",
    "Source: Deep Learning with Keras, Fran√ßois Chollet, 2017"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Classifying Newswires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the reuters dataset\n",
    "from tensorflow.keras.datasets import reuters\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)\n",
    "\n",
    "# As with the IMDB dataset, the argument num_words=10000 restricts the data to the\n",
    "# 10,000 most frequently occurring words found in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each data point is just a list of indexes of the top 10000 frequent words\n",
    "train_data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoding a encoded newswire data sample\n",
    "word_index = reuters.get_word_index()\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_newswire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective: Transform this list into a \"bag of word\" model\n",
    "# The students that did not participate in AA: https://en.wikipedia.org/wiki/Bag-of-words_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"resources/img1.png\" width=\"350\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Transform to 10.000 Dimension Vector Space with a very simply bag of words approach\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1. # This is a very simple bag of words model \n",
    "    return results\n",
    "\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range of training labels => 46 Topics\n",
    "print(\"min: {} - max: {}\".format(train_labels.min(),train_labels.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our training data is categorical, we have to transform it with one-hot-encoding into a proper format\n",
    "# basically this creates dummy variables for each category\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "one_hot_train_labels = to_categorical(train_labels)\n",
    "one_hot_test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Deep Neural Network Architecture\n",
    "The problem at hand looks very similar to the problem we solved last week. However, instead of having 2 classes (positive and negative sentiment) we do have 46 classes. Thus, the dimensionality of the output space is much larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# The raw network architecture\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note two things here:\n",
    "1. Each input vector will be mapped to a 46d output vector\n",
    "2. Last layer uses a softmax activation function. In other words, the present network will output a probability distribution "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The loss function\n",
    "The best loss function to use in this case is categorical_crossentropy. It measures\n",
    "the distance between two probability distributions: here, between the probability distribution\n",
    "output by the network and the true distribution of the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validating our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pick 1000 samples to use as a validation set\n",
    "x_val = x_train[:1000]\n",
    "partial_x_train = x_train[1000:]\n",
    "y_val = one_hot_train_labels[:1000]\n",
    "partial_y_train = one_hot_train_labels[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Phase with 20 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation data = Data on which to evaluate the loss and any model metrics at the end of each epoch.\n",
    "#                   The model will not be trained on this data.\n",
    "history = model.fit(partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the training and validation loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the traning and validation accuracy\n",
    "plt.clf()\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Experiment with layers\n",
    "\n",
    "We have a output layer with 46 nodes. What happens to the accuracy when we reduce the number of nodes the second intermediate layer\n",
    "to 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Experiment with bag of words model\n",
    "The most basic bag of words model we used assigned a 1 to any word that is in the article, but it doesn't take into account **frequencies**.\n",
    "\n",
    "Can you think of a model that takes into account word frequencies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take-Home Message:\n",
    "\n",
    "1. N Classes => N Output Nodes\n",
    "2. Output Layer should be a SoftMAX Activation function (provided that you want to a assign each data point to ONE class)\n",
    "3. Categorical Crossentropy is in many cases the loss function you should use for classification\n",
    "4. Avoid Information Bottlenecks (i.e., don't use hidden layers with too few nodes)\n",
    "5. Pre-processing inputs in a clever way can be more important than network tuning!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Regression with Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import boston_housing\n",
    "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traning Data\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical Targets \n",
    "train_targets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the values (center around 0, std of 1)\n",
    "mean = train_data.mean(axis=0)\n",
    "train_data -= mean\n",
    "std = train_data.std(axis=0)\n",
    "train_data /= std\n",
    "\n",
    "test_data -= mean\n",
    "test_data /= std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Building the network\n",
    "\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64, activation='relu', input_shape=(train_data.shape[1],)))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    # MSE = Mean Squared Error\n",
    "    # MAE = Mean Absolut Error\n",
    "    # RMSPROP adaptive learning method based on Stochastic Gradient Descent\n",
    "    # If you use SGD, your network might not converge....\n",
    "    opt = RMSprop(lr=0.001)\n",
    "    model.compile(optimizer=opt, loss='mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introducing cross validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have so little data, the variance of the validation set might be high. To cope, we use k-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "k = 4\n",
    "num_val_samples = len(train_data) // k # returns an integer instead of float\n",
    "num_epochs = 100\n",
    "all_scores = []\n",
    "\n",
    "for i in range(k):\n",
    "    print('processing fold #', i) \n",
    "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]    # Slice Get Validation Data \n",
    "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples] # Slice Val. Target Data\n",
    "    \n",
    "    # Exclude validation data from the training data\n",
    "    partial_train_data = np.concatenate(\n",
    "        [\n",
    "            train_data[:i * num_val_samples],\n",
    "            train_data[(i + 1) * num_val_samples:]\n",
    "        ],\n",
    "        axis=0)\n",
    "    partial_train_targets = np.concatenate(\n",
    "        [\n",
    "            train_targets[:i * num_val_samples],\n",
    "            train_targets[(i + 1) * num_val_samples:]\n",
    "        ],\n",
    "        axis=0)\n",
    "    \n",
    "    # Build Model\n",
    "    model = build_model()\n",
    "    \n",
    "    # Fit Model\n",
    "    model.fit(partial_train_data, partial_train_targets, epochs=num_epochs, batch_size=1, verbose=0)\n",
    "    \n",
    "    # Evaluate Model\n",
    "    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n",
    "    \n",
    "    # Add Mean Absolut Error to All Scored List\n",
    "    all_scores.append(val_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get MAE for each k-fold set\n",
    "all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Average\n",
    "np.mean(all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, lets analyze how the validation error depends on the number of epochs \n",
    "# Rerun..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "k = 2\n",
    "num_val_samples = len(train_data) // k # returns an integer instead of float\n",
    "num_epochs = 500\n",
    "all_mae_histories = [] # <-- This is changed\n",
    "\n",
    "for i in range(k):\n",
    "    print('processing fold #', i) \n",
    "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]    # Slice Get Validation Data \n",
    "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples] # Slice Val. Target Data\n",
    "    \n",
    "    # Exclude validation data from the training data\n",
    "    partial_train_data = np.concatenate(\n",
    "        [\n",
    "            train_data[:i * num_val_samples],\n",
    "            train_data[(i + 1) * num_val_samples:]\n",
    "        ],\n",
    "        axis=0)\n",
    "    partial_train_targets = np.concatenate(\n",
    "        [\n",
    "            train_targets[:i * num_val_samples],\n",
    "            train_targets[(i + 1) * num_val_samples:]\n",
    "        ],\n",
    "        axis=0)\n",
    "    \n",
    "    # Build Model\n",
    "    model = build_model()\n",
    "    \n",
    "    # Fit Model # <-- This is changed\n",
    "    history = model.fit(partial_train_data, partial_train_targets, epochs=num_epochs, batch_size=1, verbose=0)\n",
    "    \n",
    "    # Cache MAE History  # <-- This is changed\n",
    "    mae_history = history.history['mae']  \n",
    "    \n",
    "    # Add Mean Absolut Error to All Scored List # <-- This is changed\n",
    "    all_mae_histories.append(mae_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mae_histories[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot MAE History\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(1, len(all_mae_histories[0]) + 1), all_mae_histories[0])\n",
    "plt.plot(range(1, len(all_mae_histories[0]) + 1), all_mae_histories[1])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each iteration generated a history object w\n",
    "average_mae_history = [np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot average MAE History\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the Model with the Test Set\n",
    "model = build_model()\n",
    "model.fit(train_data, train_targets, epochs=80, batch_size=16, verbose=0)\n",
    "test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voila.\n",
    "test_mae_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take-Home Message\n",
    "1. Mean squared error (MSE) is a loss function commonly used for regression.\n",
    "2. A common regression metric is mean absolute error.\n",
    "3. When features in the input data have values in different ranges, each feature should be scaled independently as a preprocessing step.\n",
    "4. When there is little data available, using K-fold validation is a great way to reliably evaluate a model.\n",
    "5. If there is little data, use small network. Otherwise your network might overfit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
