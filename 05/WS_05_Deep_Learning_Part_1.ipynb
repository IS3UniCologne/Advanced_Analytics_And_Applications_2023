{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Content\n",
    "\n",
    "1. Mathematical Building Blocks of a Neuronal Network\n",
    "    1. Tensors and Tensor Operations    \n",
    "    \n",
    "**Source**: Deep Learning with Python, Francois Chollet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing Keras as part of Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```pip install tensorflow```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. First Look at a neuronal network with Keras\n",
    "In the next section, we’ll review each element in the example and explain them in detail. So don’t worry if\n",
    "some steps seem arbitrary or look like magic to you!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem to solve: Classify hand written digits using the MNIST dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s a set of 60,000 training\n",
    "images, plus 10,000 test images, assembled by the National Institute of Standards and\n",
    "Technology (the NIST in MNIST) in the 1980s. You can think of “solving” MNIST as the\n",
    "“Hello World” of deep learning—it’s what you do to verify that your algorithms are\n",
    "working as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"resources/img1.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "from tensorflow.keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eyeball data\n",
    "The images are encoded as Numpy arrays, and the labels are an array of digits, ranging\n",
    "from 0 to 9. The images and labels have a one-to-one correspondence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Data\n",
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Data\n",
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Our first neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 16:22:03.028390: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "network = models.Sequential()\n",
    "\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Who can describe this network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of the network**: Here, our network consists of a sequence of two Dense layers, which are densely\n",
    "connected (also called fully connected) neural layers. The second (and last) layer is a\n",
    "10-way softmax layer, which means it will return an array of 10 probability scores (summing\n",
    "to 1). Each score will be the probability that the current digit image belongs to\n",
    "one of our 10 digit classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make the network ready for training, we need to pick three more things, as part of the compilation step:\n",
    "# A loss function\n",
    "# an optimizer\n",
    "# metric to monitor during training and testing: for example Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All kind of losses tailored for your needs: https://keras.io/api/losses/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available optimizers: https://keras.io/api/optimizers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And metrics: https://keras.io/api/metrics/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.compile(optimizer='SGD',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing the input data**\n",
    "Before training, we’ll preprocess the data by reshaping it into the shape the network\n",
    "expects and scaling it so that all values are in the [0, 1] interval. Previously, our training\n",
    "images, for instance, were stored in an array of shape (60000, 28, 28) of type\n",
    "uint8 with values in the [0, 255] interval. We transform it into a float32 array of\n",
    "shape (60000, 28 * 28) with values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the labels\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 1.1076 - accuracy: 0.7484\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5246 - accuracy: 0.8713\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4207 - accuracy: 0.8903\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3732 - accuracy: 0.8993\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3440 - accuracy: 0.9058\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb8503366a0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Traing the network\n",
    "network.fit(train_images, train_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 952us/step - loss: 0.3178 - accuracy: 0.9148\n"
     ]
    }
   ],
   "source": [
    "# Get the metrics\n",
    "test_loss, test_acc = network.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.9124000072479248\n"
     ]
    }
   ],
   "source": [
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_err: 0.31861579418182373\n"
     ]
    }
   ],
   "source": [
    "print('test_err:', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And kid's that how we met our first artificial neural network!\n",
    "# Let's go into the details of every moving piece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Representation for Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At its core, a tensor is a container for data—almost always numerical data. So, it’s a\n",
    "container for numbers. You may be already familiar with matrices, which are 2D tensors:\n",
    "tensors are a generalization of matrices to an arbitrary number of dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(12)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0D tensors\n",
    "import numpy as np\n",
    "x = np.array(12)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12,  3,  6, 14])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1D tensors\n",
    "x = np.array([12, 3, 6, 14])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5, 78,  2, 34,  0],\n",
       "       [ 6, 79,  3, 35,  1],\n",
       "       [ 7, 80,  4, 36,  2]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2D tensors\n",
    "x = np.array([[5, 78, 2, 34, 0],\n",
    "[6, 79, 3, 35, 1],\n",
    "[7, 80, 4, 36, 2]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and so on... 3D, 4D tensors..\n",
    "x = np.array([[[5, 78, 2, 34, 0],\n",
    "[6, 79, 3, 35, 1],\n",
    "[7, 80, 4, 36, 2]],\n",
    "[[5, 78, 2, 34, 0],\n",
    "[6, 79, 3, 35, 1],\n",
    "[7, 80, 4, 36, 2]],\n",
    "[[5, 78, 2, 34, 0],\n",
    "[6, 79, 3, 35, 1],\n",
    "[7, 80, 4, 36, 2]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[5 2]\n",
      " [1 3]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "x = tf.constant([[5, 2], [1, 3]])\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key attributes of tensors:\n",
    "\n",
    "1. number of axes: ndim \n",
    "2. shape: tuple of integers that describes how many dimensions the tensor has along each axis\n",
    "3. data type: type of the data contained in the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the data \n",
    "from tensorflow.keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(train_images.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(train_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uint8\n"
     ]
    }
   ],
   "source": [
    "print(train_images.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "digit = train_images[4] # 5th element\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(digit, cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what we have here is a 3D tensor of 8-bit integers. More precisely, it’s an array of\n",
    "60,000 matrices of 28 × 8 integers. Each such matrix is a grayscale image, with coefficients\n",
    "between 0 and 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Selecting with tensors in Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_slice = train_images[10:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 28, 28)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selected 90 hand-written digits\n",
    "my_slice.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results in the same slice\n",
    "my_slice = train_images[10:100, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or more verbose\n",
    "my_slice = train_images[10:100, 0:28, 0:28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but we could focus only on a subset of the pixel for each picture (lower right part of the picture)\n",
    "my_slice = train_images[:, 14:, 14:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 14, 14)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_slice.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "digit = my_slice[4]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(digit, cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or, like negative indices in Python lists\n",
    "my_slice = train_images[:, 7:-7, 7:-7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAANuElEQVR4nO3df4xddZnH8fdDaxGqDZRWI22zA4GwS4i7mEZQiRBRWrEw/rGQEiDtSth/2LWKBCHQmCVAlmiMNisaggIBAiSIQIjaNqiUJWtD+RG2pShdFKiMzFCDFiw/Jjz7x1w2w9CCe77nnrn2+34lk7m/nnmemcwn595zz7nfyEwk7f32me4BJHXDsEuVMOxSJQy7VAnDLlViZpfN5s2bl0NDQ1223Cu8/PLLjWt37NhR1Pull14qqt+1a1dRfYmFCxc2rp01a1ZR7507dzauPeiggxrXjoyM8OKLL8bu7us07ENDQ2zatKnLlnuFjRs3Nq698cYbi3pv2LChqH7z5s1F9SXOP//8xrUHH3xwUe/777+/ce3ZZ5/duHblypV7vM+n8VIlDLtUCcMuVaIo7BGxNCJ+FRHbIuKitoaS1L7GYY+IGcB3gM8CRwJnRMSRbQ0mqV0lW/aPAtsy86nMfA24FRhuZyxJbSsJ+wLg2UnXt/due4uI+OeI2BQRm8bGxgraSSpREvbdvXH/tvNlM/OazFycmYvnz59f0E5SiZKwbwcWTbq+EHiubBxJ/VIS9geBwyPikIiYBSwH7m5nLElta3y4bGaOR8S/AGuBGcAPMnNLa5NJalXRsfGZ+WPgxy3NIqmPPIJOqoRhlyrR6SmutbrtttuK6letWtW4tvTYhtJPHz7hhBMa177wwgtFvS+44IKi+hIlf7eS3/v555/f431u2aVKGHapEoZdqoRhlyph2KVKGHapEoZdqoRhlyph2KVKGHapEoZdqoRhlyph2KVKGHapEtWc4jo+Pl5U/+CDDzauPffcc4t6lyzZfPzxxxf1Xr16dVH9cccd17j21VdfLep9+umnN65du3ZtUe8Sixcvblz7wAMP7PE+t+xSJQy7VAnDLlXCsEuVKFnFdVFE/DwitkbEloho/kFpkvquZG/8OPCVzHw4It4PPBQR6zPz8ZZmk9Sixlv2zBzJzId7l3cCW9nNKq6SBkMrr9kjYgg4Gti4m/tcslkaAMVhj4j3AT8EvpSZf5p6v0s2S4OhKOwR8R4mgn5zZt7RzkiS+qFkb3wA3we2ZuY32xtJUj+UbNk/AZwNfCoiHu19ndzSXJJaVrI++38C0eIskvrII+ikShh2qRLVnM9+0003FdWfc845LU3y/3fSSSc1ri1dLnrOnDlF9SVKZ5/Oc9IXLVrUuHbFihWNa9/p/9wtu1QJwy5VwrBLlTDsUiUMu1QJwy5VwrBLlTDsUiUMu1QJwy5VwrBLlTDsUiUMu1QJwy5V4q/qFNdLL720ce2VV15Z1HviI/eaOe+884p6X3755Y1rp/MU1VJXXHHFdI/Q2Jo1axrXlnwK88yZe460W3apEoZdqoRhlyph2KVKtLH804yIeCQi7mljIEn90caWfRUTK7hKGmCla70tBD4HXNvOOJL6pXTL/i3gQuCNPT3AJZulwVCysOMyYDQzH3qnx7lkszQYShd2PDUifgvcysQCj2UrMUjqm8Zhz8yLM3NhZg4By4GfZeZZrU0mqVW+zy5VopUTYTLzF8Av2vhZkvrDLbtUCcMuVaLT89lHRka47LLLGteXnJO+7777Nq4FWLJkSePaq666qqj3fvvtV1Rf4pVXXimqX7duXePap59+uqh3ZjauXb16dVHv4eHhovp+cMsuVcKwS5Uw7FIlDLtUCcMuVcKwS5Uw7FIlDLtUCcMuVcKwS5Uw7FIlDLtUCcMuVcKwS5Xo9BTX0dFRrr766sb1Jcsml5yiCnDnnXcW1U+Xbdu2FdWfeeaZRfWbNm0qqi9x2mmnNa698MILW5xkMLhllyph2KVKGHapEoZdqkTpwo4HRMTtEfFERGyNiI+1NZikdpXujf828NPM/MeImAXs38JMkvqgcdgjYg7wSWAlQGa+BrzWzliS2lbyNP5QYAy4LiIeiYhrI2L21AdNXrL5jTf2uLKzpD4rCftM4CPAdzPzaOBl4KKpD5q8ZPM++7g/UJouJenbDmzPzI2967czEX5JA6hkyebfA89GxBG9m04EHm9lKkmtK90b/6/Azb098U8B/1Q+kqR+KAp7Zj4KLG5nFEn95B4zqRKGXapEp+ezj4+PMzY21mXL/7NmzZqi+tHR0ca11113XVHvu+66q3Htli1binrv3LmzqL7kMwhK36o966yzGtfOnv22Q0b+6rlllyph2KVKGHapEoZdqoRhlyph2KVKGHapEoZdqoRhlyph2KVKGHapEoZdqoRhlyph2KVKGHapEp2ezz5z5kzmzZvXuL7knPKhoaHGtVB2XvZ0WrBgQVH9nDlziuqfe+65xrUl/ysAp5xySlH93sYtu1QJwy5VwrBLlShdsvnLEbElIjZHxC0R8d62BpPUrsZhj4gFwBeBxZl5FDADWN7WYJLaVfo0fiawX0TMZGJt9ua7XiX1Vclab78DvgE8A4wAf8zMdVMf55LN0mAoeRp/IDAMHAIcDMyOiLd9ULdLNkuDoSR9nwZ+k5ljmfk6cAfw8XbGktS2krA/AxwbEfvHxOFlJwJb2xlLUttKXrNvBG4HHgb+u/ezrmlpLkktK12y+WvA11qaRVIfucdMqoRhlyrR6Smuhx12GNdff33j+mXLljWu3bFjR+NamJi9qeHh4aLeK1eubFw7d+7cot7Ll5cdFFlyimtpb72VW3apEoZdqoRhlyph2KVKGHapEoZdqoRhlyph2KVKGHapEoZdqoRhlyph2KVKGHapEoZdqoRhlyrR6fnss2fP5phjjmlcPzY21uI0ddiwYUNR/X333VdUX7LU9aGHHlrUW2/lll2qhGGXKmHYpUq8a9gj4gcRMRoRmyfdNjci1kfEk73vB/Z3TEml/pIt+/XA0im3XQTcm5mHA/f2rksaYO8a9szcAPxhys3DwA29yzcAn293LElta/qa/YOZOQLQ+/6BPT1w8pLNvnUmTZ++76CbvGTz/Pnz+91O0h40DfvzEfEhgN730fZGktQPTcN+N7Cid3kFcFc740jql7/krbdbgP8CjoiI7RFxDvDvwGci4kngM73rkgbYux4bn5ln7OGuE1ueRVIfeQSdVAnDLlWi01Nc1b1du3YV1Zecolpa75LN7XLLLlXCsEuVMOxSJQy7VAnDLlXCsEuVMOxSJQy7VAnDLlXCsEuVMOxSJQy7VAnDLlXCsEuVMOxSJTyffS+3ZMmS6R5BA8Itu1QJwy5VwrBLlWi6ZPPXI+KJiHgsIn4UEQf0dUpJxZou2bweOCozPwz8Gri45bkktazRks2ZuS4zx3tXfwks7MNsklrUxmv2LwA/aeHnSOqjorBHxCXAOHDzOzzG9dmlAdA47BGxAlgGnJmZuafHuT67NBgaHUEXEUuBrwLHZ+af2x1JUj80XbL5P4D3A+sj4tGI+F6f55RUqOmSzd/vwyyS+sgj6KRKGHapEp7iupdbu3btdI+gAeGWXaqEYZcqYdilShh2qRKGXaqEYZcqYdilShh2qRKGXaqEYZcqYdilShh2qRKGXaqEYZcqYdilSsQ7fDBs+80ixoCn3+Eh84AXOhrH3vbeG3v/TWbu9mOcOw37u4mITZm52N72tnf7fBovVcKwS5UYtLBfY29727s/Buo1u6T+GbQtu6Q+MexSJQYi7BGxNCJ+FRHbIuKiDvsuioifR8TWiNgSEau66j1phhkR8UhE3NNx3wMi4vaIeKL3+3+sw95f7v29N0fELRHx3j73+0FEjEbE5km3zY2I9RHxZO/7gR32/nrv7/5YRPwoIg7oR++ppj3sETED+A7wWeBI4IyIOLKj9uPAVzLz74BjgfM67P2mVcDWjnsCfBv4aWb+LfD3Xc0QEQuALwKLM/MoYAawvM9trweWTrntIuDezDwcuLd3vave64GjMvPDwK+Bi/vU+y2mPezAR4FtmflUZr4G3AoMd9E4M0cy8+He5Z1M/MMv6KI3QEQsBD4HXNtVz17fOcAn6S3QmZmvZeaLHY4wE9gvImYC+wPP9bNZZm4A/jDl5mHght7lG4DPd9U7M9dl5njv6i+Bhf3oPdUghH0B8Oyk69vpMHBviogh4GhgY4dtvwVcCLzRYU+AQ4Ex4LreS4hrI2J2F40z83fAN4BngBHgj5m5roveU3wwM0d6M40AH5iGGQC+APyki0aDEPbYzW2dvh8YEe8Dfgh8KTP/1FHPZcBoZj7URb8pZgIfAb6bmUcDL9O/p7Fv0XttPAwcAhwMzI6Is7roPWgi4hImXkre3EW/QQj7dmDRpOsL6fPTuski4j1MBP3mzLyjq77AJ4BTI+K3TLx0+VRE3NRR7+3A9sx881nM7UyEvwufBn6TmWOZ+TpwB/DxjnpP9nxEfAig9320y+YRsQJYBpyZHR3sMghhfxA4PCIOiYhZTOysubuLxhERTLxu3ZqZ3+yi55sy8+LMXJiZQ0z8zj/LzE62cJn5e+DZiDiid9OJwONd9Gbi6fuxEbF/7+9/ItOzg/JuYEXv8grgrq4aR8RS4KvAqZn55676kpnT/gWczMReyf8BLumw73FMvGR4DHi093XyNPz+JwD3dNzzH4BNvd/9TuDADnv/G/AEsBm4Edi3z/1uYWL/wOtMPKs5BziIib3wT/a+z+2w9zYm9lO9+T/3vS7+7h4uK1ViEJ7GS+qAYZcqYdilShh2qRKGXaqEYZcqYdilSvwvIg36wwHgosQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "digit = my_slice[4]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(digit, cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The notion of data batches\n",
    "In general, the first axis (axis 0, because indexing starts at 0) in all data tensors you’ll\n",
    "come across in deep learning will be the samples axis (sometimes called the samples\n",
    "dimension). In the MNIST example, samples are images of digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example batching for the n-th batch\n",
    "n = 3\n",
    "batch = train_images[128 * n:128 * (n + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 28, 28)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real world examples of tensors\n",
    "1. vector data : 2d tensors (sample, features)\n",
    "2. timeseries: 3d tensors (sample, timestep, feature)\n",
    "3. image: 4d tensors (sample, height, width, channel (color)) \n",
    "4. video: 5d tensors (sample, frames, height, width, channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timeseries\n",
    "<img src=\"resources/img2.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video Frames\n",
    "<img src=\"resources/img3.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras.layers.Dense(512, activation='relu') \n",
    "# equals\n",
    "# output = relu(dot(W, input) + b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elementwise operations allow us to parallize many computations.\n",
    "# The elementwise operations are implemented with numpy array, so nothing changes here.\n",
    "\n",
    "# Example \n",
    "def naive_add(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert x.shape == y.shape\n",
    "    \n",
    "    x = x.copy()\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] += y[i, j]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Broadcasting\n",
    "We will encouter many situation in which we want to work with different dimensioned vectors, \n",
    "however our naive approach with numpy does not work here. Thus, keras supports a method to \"broadcast\" the dimension - i.e., artificially match the dimension of vectors without creating objects in the memory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor Dot \n",
    "Computes the dot product betweet two tensors - e.g., two vectors -> inner product\n",
    "\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html\n",
    "\n",
    "**Not** to be confused with the elementwise product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example tensor dot computation\n",
    "import numpy as np\n",
    "z = np.dot(x, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor Reshaping\n",
    "Finally, tensor reshaping. We have already used this method in previous workshops to change the shape of arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28 * 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Task - Create a Neural Network which can identify whether a number is 4 or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What needs to change? Which activation function at the output node? Which loss function to take?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1728 - accuracy: 0.9397\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0903 - accuracy: 0.9735\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0740 - accuracy: 0.9781\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0662 - accuracy: 0.9801\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0615 - accuracy: 0.9815\n",
      "313/313 [==============================] - 0s 914us/step - loss: 0.0601 - accuracy: 0.9812\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "# Create Binary Case\n",
    "train_labels = np.where(train_labels==4, 1, 0)\n",
    "test_labels = np.where(test_labels==4, 1, 0)\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "# Build Network\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile network \n",
    "network.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the network\n",
    "network.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
    "\n",
    "# Get the metrics\n",
    "test_loss, test_acc = network.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2043 - accuracy: 0.9218\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1124 - accuracy: 0.9664\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0887 - accuracy: 0.9741\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0775 - accuracy: 0.9774\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0708 - accuracy: 0.9790\n",
      "313/313 [==============================] - 0s 909us/step - loss: 0.0683 - accuracy: 0.9790\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "# Create Binary Case\n",
    "train_labels = np.where(train_labels==4, 1, 0)\n",
    "test_labels = np.where(test_labels==4, 1, 0)\n",
    "\n",
    "# Build Network\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile network \n",
    "network.compile(optimizer='SGD', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the network\n",
    "network.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
    "\n",
    "# Get the metrics\n",
    "test_loss, test_acc = network.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Sentiment Analysis with Neural Networks - Classifying movie reviews "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMDB dataset: a set of 50,000 highly polarized reviews from the\n",
    "Internet Movie Database. They’re split into 25,000 reviews for training and 25,000\n",
    "reviews for testing, each set consisting of 50% negative and 50% positive reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17464789/17464789 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# loading the data\n",
    "from tensorflow.keras.datasets import imdb\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n",
    "num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The argument num_words=10000 means you’ll only keep the top 10,000 most frequently\n",
    "occurring words in the training data. Rare words will be discarded. This allows\n",
    "you to work with vector data of manageable size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 2,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 2,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 2,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 5244,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 5952,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 2,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 7486,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 5535,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 5345,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Represents a review.\n",
    "# Each word is encoded (for example the word \"boring\" could have the numerical code 17)\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Label\n",
    "# 0 stands for negative, 1 for positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1641221/1641221 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Decoding a review for illustration purpose\n",
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = dict(\n",
    "[(value, key) for (key, value) in word_index.items()])\n",
    "decoded_review = ' '.join(\n",
    "[reverse_word_index.get(i - 3, '?') for i in train_data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print decoded review\n",
    "decoded_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming input data to suitable format \n",
    "import numpy as np\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task: \n",
    "Develop a Deep Neural Network with Input Layer, Hidden Layer (16 Nodes), Hidden Layer (16 Nodes) \n",
    "and 1 Output. \n",
    "\n",
    "1. Use RELU for the hidden layer activations and a sigmoid actitvation function for the output layer.\n",
    "2. Use Stochastic Gradient Descent to optimize the network\n",
    "3. Select an appropriate loss function and metric \n",
    "\n",
    "\n",
    "What happens when you increase the epoch (from perhaps 2 to 10)? \n",
    "Also, analyze what happens when you vary the batch size?\n",
    "Analyze what happens when you add one more hidden layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1563/1563 [==============================] - 1s 725us/step - loss: 0.4522 - accuracy: 0.8039\n",
      "Epoch 2/2\n",
      "1563/1563 [==============================] - 1s 652us/step - loss: 0.2830 - accuracy: 0.8840\n",
      "782/782 [==============================] - 1s 658us/step - loss: 0.2904 - accuracy: 0.8814\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer='SGD', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train Model\n",
    "model.fit(x_train, train_labels, epochs=2, batch_size=16)\n",
    "\n",
    "results = model.evaluate(x_test, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.29042935371398926, 0.8814399838447571]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print results (loss, accuracy)\n",
    "results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1563/1563 [==============================] - 1s 716us/step - loss: 0.4422 - accuracy: 0.8061\n",
      "Epoch 2/2\n",
      "1563/1563 [==============================] - 1s 648us/step - loss: 0.2833 - accuracy: 0.8849\n",
      "782/782 [==============================] - 1s 667us/step - loss: 0.3031 - accuracy: 0.8755\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer='SGD', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train Model\n",
    "model.fit(x_train, train_labels, epochs=2, batch_size=16)\n",
    "\n",
    "results = model.evaluate(x_test, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3030532896518707, 0.8755199909210205]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print results (loss, accuracy)\n",
    "results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25/25 [==============================] - 1s 14ms/step - loss: 0.6936 - accuracy: 0.4992\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 0s 12ms/step - loss: 0.6914 - accuracy: 0.5287\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 0s 12ms/step - loss: 0.6888 - accuracy: 0.5573\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 0.6848 - accuracy: 0.5902\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 0s 12ms/step - loss: 0.6781 - accuracy: 0.6273\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 0s 12ms/step - loss: 0.6686 - accuracy: 0.6646\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 0.6573 - accuracy: 0.7000\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 0.6449 - accuracy: 0.7270\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 0.6316 - accuracy: 0.7494\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 0.6175 - accuracy: 0.7650\n",
      "782/782 [==============================] - 1s 638us/step - loss: 0.6147 - accuracy: 0.7600\n"
     ]
    }
   ],
   "source": [
    "# Less backprop steps\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer='SGD', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train Model\n",
    "model.fit(x_train, train_labels, epochs=10, batch_size=1024)\n",
    "\n",
    "results = model.evaluate(x_test, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.61473548412323, 0.7599999904632568]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Results\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/9\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 0.4165 - accuracy: 0.8172\n",
      "Epoch 2/9\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 0.2736 - accuracy: 0.8893\n",
      "Epoch 3/9\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 0.2297 - accuracy: 0.9086\n",
      "Epoch 4/9\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 0.2007 - accuracy: 0.9223\n",
      "Epoch 5/9\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 0.1747 - accuracy: 0.9334\n",
      "Epoch 6/9\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 0.1496 - accuracy: 0.9445\n",
      "Epoch 7/9\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 0.1255 - accuracy: 0.9548\n",
      "Epoch 8/9\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 0.1022 - accuracy: 0.9651\n",
      "Epoch 9/9\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 0.0794 - accuracy: 0.9740\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.3710 - accuracy: 0.8701\n"
     ]
    }
   ],
   "source": [
    "# A model which might be prone to overfitting\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(256, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer='SGD', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train Model\n",
    "model.fit(x_train, train_labels, epochs=10, batch_size=16)\n",
    "\n",
    "results = model.evaluate(x_test, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.37103813886642456, 0.8701199889183044]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# overfitted results\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few last words: using a trained network to generate predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-17 13:39:05.407256: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.08382644],\n",
       "       [0.9999944 ],\n",
       "       [0.6880363 ],\n",
       "       ...,\n",
       "       [0.1475069 ],\n",
       "       [0.01761602],\n",
       "       [0.9954562 ]], dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_test) # <- Here, you can pass any data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UBS",
   "language": "python",
   "name": "my-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
